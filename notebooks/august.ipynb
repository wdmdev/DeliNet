{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# August's Notebook for exploring the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project description\n",
    "Food is an important part of our lives. Imagine an AI agent that can look at a dish, recognize\n",
    "ingredients, and reliably reconstruct the exact recipe of the dish, or another agent that can read,\n",
    "interpret, and execute a cooking recipe to produce our favorite meal. Computer vision community\n",
    "has long studied image-level food classification [1, 2, 7, 8, 9, 11], and only recently focused on\n",
    "understanding the mapping between recipes and images using multi-modal representations [5, 10,\n",
    "13, 14, 18].\n",
    "Inspired by CLIP [12], the goal of this project is to retrieve a recipe (from a list of known recipes)\n",
    "given an image query and, in reverse, to retrieve an image (from a list of known images) given a\n",
    "text recipe. For this, the team will work on text and image retrieval by combining both modali-\n",
    "ties. In addition, the team will explore several additional textual information (title, instructions,\n",
    "ingredients) and analyze their impact.\n",
    "\n",
    "## Data\n",
    "In this project, you will use the Food Ingredients and Recipes Dataset from kaggle 1. The dataset\n",
    "consists of 13,582 images and each image comes with a corresponding title (the title of the food\n",
    "dish), a list of ingredients (the ingredients as they were scraped from the website), and a list of\n",
    "instructions (the recipe instructions to be followed to recreate the dish.)\n",
    "\n",
    "## Tasks\n",
    "In this project, you could work on the following tasks:\n",
    "\n",
    "### Task 1: Image-to-recipe retrieval task. \n",
    "In this task, you are asked to build a model that is\n",
    "able to perform the image-to-recipe retrieval task. The model should consist of an image encoder\n",
    "(based on a standard CNN architecture [6, 16, 15] or even a visual transformer [4]) and a text\n",
    "encoder (based on a text transformer [17] or a BERT model [3]). You can get inspiration from the\n",
    "popular CLIP model from OpenAI [12]. The model should be trained with a triplet or a contrastive\n",
    "loss to learn a a joint embedding of text recipes and food images.\n",
    "\n",
    "### Task 2: Additional text modalities. \n",
    "In this task, you are asked to build on top of the\n",
    "model of Task 1 by adding extra text modalities (instructions and ingredients) when training\n",
    "the image-text model. You can either simply concatenate all text (title, title+ingredients, ti-\n",
    "tle+ingredients+instructions) or consider more advanced ways such as using one transformer for\n",
    "each text element (eg. BERT [3]) and then concatenate the features for all text modalities (note,\n",
    "you may need to project everything to a common feature space).\n",
    "\n",
    "### Task 3: Compare results with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "from transformers import BertModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1: Solve the image-to-recipe retrieval task\n",
    "\"\"\"\n",
    "In this task, you are asked to build a model that is able to perform the image-to-recipe retrieval task. \n",
    "The model should consist of an image encoder (based on a standard CNN architecture [6, 16, 15] \n",
    "or even a visual transformer [4]) and a text encoder (based on a text transformer [17] or a BERT model [3]). \n",
    "You can get inspiration from the popular CLIP model from OpenAI [12]. \n",
    "The model should be trained with a triplet or a contrastive loss to learn a a joint embedding of text recipes and food images.\n",
    "\"\"\"\n",
    "\n",
    "# Define the image encoder\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = resnet50(pretrained=True)\n",
    "        self.fc = nn.Linear(self.cnn.fc.in_features, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn.conv1(x)\n",
    "        x = self.cnn.bn1(x)\n",
    "        x = self.cnn.relu(x)\n",
    "        x = self.cnn.maxpool(x)\n",
    "        x = self.cnn.layer1(x)\n",
    "        x = self.cnn.layer2(x)\n",
    "        x = self.cnn.layer3(x)\n",
    "        x = self.cnn.layer4(x)\n",
    "        x = self.cnn.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define the text encoder\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 512)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = self.fc(outputs.pooler_output)\n",
    "        return x\n",
    "\n",
    "# Define the model\n",
    "class ImageRecipeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_emb = self.image_encoder(image)\n",
    "        text_emb = self.text_encoder(input_ids, attention_mask)\n",
    "        return image_emb, text_emb\n",
    "\n",
    "# Define the contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, image_emb, text_emb):\n",
    "        distance = (image_emb - text_emb).pow(2).sum(1)\n",
    "        loss = torch.clamp(self.margin - distance, min=0)\n",
    "        return loss.mean()\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
