{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# August's Notebook for exploring the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project description\n",
    "Food is an important part of our lives. Imagine an AI agent that can look at a dish, recognize\n",
    "ingredients, and reliably reconstruct the exact recipe of the dish, or another agent that can read,\n",
    "interpret, and execute a cooking recipe to produce our favorite meal. Computer vision community\n",
    "has long studied image-level food classification [1, 2, 7, 8, 9, 11], and only recently focused on\n",
    "understanding the mapping between recipes and images using multi-modal representations [5, 10,\n",
    "13, 14, 18].\n",
    "Inspired by CLIP [12], the goal of this project is to retrieve a recipe (from a list of known recipes)\n",
    "given an image query and, in reverse, to retrieve an image (from a list of known images) given a\n",
    "text recipe. For this, the team will work on text and image retrieval by combining both modali-\n",
    "ties. In addition, the team will explore several additional textual information (title, instructions,\n",
    "ingredients) and analyze their impact.\n",
    "\n",
    "## Data\n",
    "In this project, you will use the Food Ingredients and Recipes Dataset from kaggle 1. The dataset\n",
    "consists of 13,582 images and each image comes with a corresponding title (the title of the food\n",
    "dish), a list of ingredients (the ingredients as they were scraped from the website), and a list of\n",
    "instructions (the recipe instructions to be followed to recreate the dish.)\n",
    "\n",
    "## Tasks\n",
    "In this project, you could work on the following tasks:\n",
    "\n",
    "### Task 1: Image-to-recipe retrieval task. \n",
    "In this task, you are asked to build a model that is\n",
    "able to perform the image-to-recipe retrieval task. The model should consist of an image encoder\n",
    "(based on a standard CNN architecture [6, 16, 15] or even a visual transformer [4]) and a text\n",
    "encoder (based on a text transformer [17] or a BERT model [3]). You can get inspiration from the\n",
    "popular CLIP model from OpenAI [12]. The model should be trained with a triplet or a contrastive\n",
    "loss to learn a a joint embedding of text recipes and food images.\n",
    "\n",
    "### Task 2: Additional text modalities. \n",
    "In this task, you are asked to build on top of the\n",
    "model of Task 1 by adding extra text modalities (instructions and ingredients) when training\n",
    "the image-text model. You can either simply concatenate all text (title, title+ingredients, ti-\n",
    "tle+ingredients+instructions) or consider more advanced ways such as using one transformer for\n",
    "each text element (eg. BERT [3]) and then concatenate the features for all text modalities (note,\n",
    "you may need to project everything to a common feature space).\n",
    "\n",
    "### Task 3: Compare results with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "from transformers import BertModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1: Solve the image-to-recipe retrieval task\n",
    "\"\"\"\n",
    "In this task, you are asked to build a model that is able to perform the image-to-recipe retrieval task. \n",
    "The model should consist of an image encoder (based on a standard CNN architecture [6, 16, 15] \n",
    "or even a visual transformer [4]) and a text encoder (based on a text transformer [17] or a BERT model [3]). \n",
    "You can get inspiration from the popular CLIP model from OpenAI [12]. \n",
    "The model should be trained with a triplet or a contrastive loss to learn a a joint embedding of text recipes and food images.\n",
    "\"\"\"\n",
    "\n",
    "# Define the image encoder\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = resnet50(pretrained=True)\n",
    "        self.fc = nn.Linear(self.cnn.fc.in_features, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn.conv1(x)\n",
    "        x = self.cnn.bn1(x)\n",
    "        x = self.cnn.relu(x)\n",
    "        x = self.cnn.maxpool(x)\n",
    "        x = self.cnn.layer1(x)\n",
    "        x = self.cnn.layer2(x)\n",
    "        x = self.cnn.layer3(x)\n",
    "        x = self.cnn.layer4(x)\n",
    "        x = self.cnn.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define the text encoder\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 512)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = self.fc(outputs.pooler_output)\n",
    "        return x\n",
    "\n",
    "# Define the model\n",
    "class ImageRecipeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_emb = self.image_encoder(image)\n",
    "        text_emb = self.text_encoder(input_ids, attention_mask)\n",
    "        return image_emb, text_emb\n",
    "\n",
    "# Define the contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, image_emb, text_emb):\n",
    "        distance = (image_emb - text_emb).pow(2).sum(1)\n",
    "        loss = torch.clamp(self.margin - distance, min=0)\n",
    "        return loss.mean()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this project, a function will be made for evaluation.\n",
    "# This function will compute a metric for comparing the self-trained model and CLIP.\n",
    "# The metric is the mean cosine similarity between the image and recipe embeddings.\n",
    "# The cosine similarity is computed as the dot product of the embeddings divided by the product of the norms of the embeddings.\n",
    "# The cosine similarity is a measure of similarity between two non-zero vectors.\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, dataloader, device, similarity='cosine', acc_percent=0.01):\n",
    "\n",
    "    ## First we compute all the embeddings for the images and recipes\n",
    "    model.eval()\n",
    "\n",
    "    # Compute the embeddings for the images\n",
    "    image_embeddings = []\n",
    "    for batch in dataloader:\n",
    "        image, _, _ = batch\n",
    "        image = image.to(device)\n",
    "        image_emb = model.image_encoder(image)\n",
    "        image_emb = image_emb / image_emb.norm(dim=1, keepdim=True)\n",
    "        image_embeddings.append(image_emb)\n",
    "\n",
    "    # Compute the embeddings for the recipes\n",
    "    recipe_embeddings = []\n",
    "    for batch in dataloader:\n",
    "        _, input_ids, attention_mask = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        recipe_emb = model.text_encoder(input_ids, attention_mask)\n",
    "        recipe_emb = recipe_emb / recipe_emb.norm(dim=1, keepdim=True)\n",
    "        recipe_embeddings.append(recipe_emb)\n",
    "\n",
    "    # Compute the mean cosine similarity between the image and recipe embeddings\n",
    "    cosine_similarity = 0\n",
    "    for image_emb, recipe_emb in zip(image_embeddings, recipe_embeddings):\n",
    "        if similarity == 'cosine':\n",
    "            cosine_similarity += (image_emb * recipe_emb).sum(dim=1).mean().item()\n",
    "        elif similarity == 'euclidean':\n",
    "            cosine_similarity += (image_emb - recipe_emb).pow(2).sum(1).mean().item()\n",
    "    \n",
    "\n",
    "    # Compute the accuracy of the model based on top acc_percent percentage of the most similar images\n",
    "    cosine_similarity /= len(dataloader)\n",
    "    if similarity == 'cosine':\n",
    "        similarity = True\n",
    "    elif similarity == 'euclidean':\n",
    "        similarity = False\n",
    "    topk = int(len(dataloader) * acc_percent)\n",
    "    accuracy = 0\n",
    "    for image_emb, recipe_emb in zip(image_embeddings, recipe_embeddings):\n",
    "        similarity_scores = []\n",
    "        for recipe in recipe_embeddings:\n",
    "            if similarity:\n",
    "                similarity_scores.append((image_emb * recipe).sum(dim=1).item())\n",
    "            else:\n",
    "                similarity_scores.append((image_emb - recipe).pow(2).sum(1).item())\n",
    "        similarity_scores = torch.tensor(similarity_scores)\n",
    "        _, topk_indices = similarity_scores.topk(topk)\n",
    "        if 0 in topk_indices:\n",
    "            accuracy += 1\n",
    "    accuracy /= len(dataloader)\n",
    "\n",
    "    return cosine_similarity, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ## Evaluate the model computing cosine similarity\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     cosine_similarity = 0\n",
    "    #     for batch in dataloader:\n",
    "    #         image, input_ids, attention_mask = batch\n",
    "    #         image = image.to(device)\n",
    "    #         input_ids = input_ids.to(device)\n",
    "    #         attention_mask = attention_mask.to(device)\n",
    "    #         image_emb, text_emb = model(image, input_ids, attention_mask)\n",
    "    #         image_emb = image_emb / image_emb.norm(dim=1, keepdim=True)\n",
    "    #         text_emb = text_emb / text_emb.norm(dim=1, keepdim=True)\n",
    "    #         cosine_similarity += (image_emb * text_emb).sum(dim=1).mean().item()\n",
    "    #     cosine_similarity /= len(dataloader)\n",
    "    # return cosine_similarity\n",
    "\n",
    "    ## Sort \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a1868ce70718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json_path' is not defined"
     ]
    }
   ],
   "source": [
    "### Dataloader as copied directly from Medium article\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    input_data = []\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        input_data.append(obj)\n",
    "\n",
    "\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "\n",
    "class image_title_dataset():\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.image_path = list_image_path\n",
    "        # Tokenize text using CLIP's tokenizer\n",
    "        self.title  = clip.tokenize(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = preprocess(Image.open(self.image_path[idx]))\n",
    "        title = self.title[idx]\n",
    "        return image, title\n",
    "    \n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "# loss_img = nn.CrossEntropyLoss()\n",
    "# loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "# num_epochs = 30\n",
    "# for epoch in range(num_epochs):\n",
    "#     pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "#     for batch in pbar:\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         images,texts = batch \n",
    "        \n",
    "#         images= images.to(device)\n",
    "#         texts = texts.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "#         # Compute loss\n",
    "#         ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "#         total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "#         # Backward pass\n",
    "#         total_loss.backward()\n",
    "#         if device == \"cpu\":\n",
    "#             optimizer.step()\n",
    "#         else : \n",
    "#             convert_models_to_fp32(model)\n",
    "#             optimizer.step()\n",
    "#             clip.model.convert_weights(model)\n",
    "\n",
    "#         pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
